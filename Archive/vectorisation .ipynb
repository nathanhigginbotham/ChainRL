{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of Vectorisation of OR_Gym Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import or_gym\n",
    "from or_gym.utils import create_env\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below. Set the number of cores of your CPU (google colab has 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"InvManagement-v1\"\n",
    "num_cpu = 2  # Number of processes to use\n",
    "\n",
    "# Create the vectorized environment (DummyVecEnv)\n",
    "vec_env = make_vec_env(env_id, n_envs=num_cpu)\n",
    "\n",
    "model = PPO('MlpPolicy', vec_env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 726.28s for multiprocessed version - 550.75 FPS\n",
      "Took 982.81s for single process version - 407.00 FPS\n",
      "Multiprocessed training is 1.35x faster!\n"
     ]
    }
   ],
   "source": [
    "n_timesteps = 400000\n",
    "\n",
    "# Timing block for Multiprocessed RL Training\n",
    "start_time = time.time()\n",
    "model.learn(n_timesteps)\n",
    "total_time_multi = time.time() - start_time\n",
    "##############################################\n",
    "\n",
    "print(f\"Took {total_time_multi:.2f}s for multiprocessed version - {n_timesteps / total_time_multi:.2f} FPS\")\n",
    "\n",
    "single_process_model = PPO('MlpPolicy', env_id, verbose=0)\n",
    "\n",
    "# Timing block for Single Process RL Training\n",
    "start_time = time.time()\n",
    "single_process_model.learn(n_timesteps)\n",
    "total_time_single = time.time() - start_time\n",
    "##############################################\n",
    "\n",
    "print(f\"Took {total_time_single:.2f}s for single process version - {n_timesteps / total_time_single:.2f} FPS\")\n",
    "\n",
    "print(\"Multiprocessed training is {:.2f}x faster!\".format(total_time_single / total_time_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -1.991901731491089 +/- 4.18\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "eval_env = gym.make(env_id)\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4612a4b7fa53cc24da9036e6d31dbb4f4c5422e33136073430e4190579d7d4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
